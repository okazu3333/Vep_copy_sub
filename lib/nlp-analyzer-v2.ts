import { LanguageServiceClient, protos } from '@google-cloud/language'
import { BigQuery } from '@google-cloud/bigquery'
import { TextDecoder } from './text-decoder'

// NLPåˆ†æçµæœã®å‹å®šç¾©
export interface NLPAnalysisResult {
  message_id: string
  thread_id: string
  subject: string
  body: string
  sentiment: {
    score: number
    magnitude: number
    label: 'POSITIVE' | 'NEGATIVE' | 'NEUTRAL'
  }
  entities: Array<{
    name: string
    type: string
    salience: number
    metadata?: Record<string, string>
  }>
  categories: Array<{
    name: string
    confidence: number
  }>
  syntax: {
    sentences: number
    tokens: number
    avg_sentence_length: number
  }
  analysis_timestamp: string
}

// ãƒãƒƒãƒåˆ†æçµæœã®å‹å®šç¾©
export interface BatchNLPAnalysisResult {
  total_processed: number
  successful_analyses: number
  failed_analyses: number
  results: NLPAnalysisResult[]
  summary: {
    sentiment_distribution: Record<string, number>
    top_entities: Array<{ name: string; count: number }>
    top_categories: Array<{ name: string; count: number }>
    avg_sentiment_score: number
    avg_magnitude: number
  }
  processing_time_ms: number
}

// æ¤œçŸ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°çµæœã®å‹å®šç¾©
export interface PatternMatchResult {
  pattern_id: string
  pattern_name: string
  confidence: number
  matched_conditions: string[]
  risk_score: number
  urgency_level: 'critical' | 'high' | 'medium' | 'low'
  business_impact: 'high' | 'medium' | 'low'
  recommended_actions: string[]
}

export class NLPAnalyzerV2 {
  private languageClient: LanguageServiceClient
  private bigquery: BigQuery

  constructor() {
    // Google Cloudèªè¨¼ã®è¨­å®š
    const projectId = process.env.GOOGLE_CLOUD_PROJECT_ID || 'viewpers'
    
    console.log('ğŸ” Google Cloudèªè¨¼è¨­å®š:', {
      projectId,
      hasCredentials: !!process.env.GOOGLE_APPLICATION_CREDENTIALS,
      useDefaultCredentials: !process.env.GOOGLE_APPLICATION_CREDENTIALS
    })

    try {
      this.languageClient = new LanguageServiceClient({
        projectId: projectId
      })
      this.bigquery = new BigQuery({
        projectId: projectId
      })
      
      console.log('âœ… Google Cloudã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†')
    } catch (error) {
      console.error('âŒ Google Cloudã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼:', error)
      throw error
    }
  }

  /**
   * ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ã‚’è¡Œã£ã¦ã‹ã‚‰NLPåˆ†æã‚’å®Ÿè¡Œ
   */
  async analyzeTextWithPreprocessing(text: string): Promise<NLPAnalysisResult | null> {
    try {
      // 1. æ–‡å­—åŒ–ã‘ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
      const preprocessedText = this.preprocessText(text)
      
      if (!preprocessedText || preprocessedText.trim().length < 10) {
        console.log('âš ï¸ å‰å‡¦ç†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã‚‹ã‹ç©ºã§ã™:', {
          originalLength: text.length,
          preprocessedLength: preprocessedText?.length || 0
        })
        return null
      }

      // 2. NLPåˆ†æå®Ÿè¡Œ
      return await this.analyzeText(preprocessedText)
    } catch (error) {
      console.error('âŒ å‰å‡¦ç†ä»˜ãNLPåˆ†æã‚¨ãƒ©ãƒ¼:', error)
      return null
    }
  }

  /**
   * ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
   */
  private preprocessText(text: string): string {
    if (!text || text.length === 0) {
      return ''
    }

    try {
      // 1. æ–‡å­—åŒ–ã‘ãƒ‡ãƒ¼ã‚¿ã®å¾©å·åŒ–
      const decodeResult = TextDecoder.decodeText(text)
      let processedText = decodeResult.success ? decodeResult.decodedText : text

      // 2. HTMLã‚¿ã‚°ã®é™¤å»
      processedText = this.removeHtmlTags(processedText)

      // 3. ç‰¹æ®Šæ–‡å­—ã®æ­£è¦åŒ–
      processedText = this.normalizeSpecialCharacters(processedText)

      // 4. ç©ºç™½ã®æ­£è¦åŒ–
      processedText = this.normalizeWhitespace(processedText)

      // 5. æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é™¤å»
      processedText = this.removeGarbledPatterns(processedText)

      console.log('ğŸ”§ ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†å®Œäº†:', {
        originalLength: text.length,
        processedLength: processedText.length,
        decodeSuccess: decodeResult.success,
        decodeConfidence: decodeResult.confidence
      })

      return processedText
    } catch (error) {
      console.error('âŒ ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã‚¨ãƒ©ãƒ¼:', error)
      return text // ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
    }
  }

  /**
   * HTMLã‚¿ã‚°ã®é™¤å»
   */
  private removeHtmlTags(text: string): string {
    return text.replace(/<[^>]*>/g, '')
  }

  /**
   * ç‰¹æ®Šæ–‡å­—ã®æ­£è¦åŒ–
   */
  private normalizeSpecialCharacters(text: string): string {
    return text
      .replace(/[^\x00-\x7F\u3000-\u303F\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\uFF00-\uFFEF]/g, '')
      .replace(/[\u0000-\u001F\u007F-\u009F]/g, '')
  }

  /**
   * ç©ºç™½ã®æ­£è¦åŒ–
   */
  private normalizeWhitespace(text: string): string {
    return text
      .replace(/\s+/g, ' ')
      .replace(/\n+/g, '\n')
      .trim()
  }

  /**
   * æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é™¤å»
   */
  private removeGarbledPatterns(text: string): string {
    return text
      .replace(/\$B[^$]*\$B/g, '') // $B...$B ãƒ‘ã‚¿ãƒ¼ãƒ³
      .replace(/[^\x00-\x7F\u3000-\u303F\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\uFF00-\uFFEF\s]/g, '') // ä¸æ­£æ–‡å­—
      .replace(/\s+/g, ' ')
      .trim()
  }

  // å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®NLPåˆ†æ
  async analyzeText(text: string): Promise<any> {
    try {
      console.log('ğŸ” NLPåˆ†æé–‹å§‹:', {
        textLength: text.length,
        textPreview: text.substring(0, 100) + '...'
      })

      const document = {
        content: text,
        type: 'PLAIN_TEXT' as const,
        language: 'ja'
      }

      console.log('ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæº–å‚™å®Œäº†:', document)

      // æ„Ÿæƒ…åˆ†æ
      console.log('ğŸ˜Š æ„Ÿæƒ…åˆ†æå®Ÿè¡Œä¸­...')
      const [sentimentResult] = await this.languageClient.analyzeSentiment({ document })
      const sentiment = sentimentResult.documentSentiment
      console.log('âœ… æ„Ÿæƒ…åˆ†æå®Œäº†:', sentiment)

      // ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åˆ†æ
      console.log('ğŸ·ï¸ ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åˆ†æå®Ÿè¡Œä¸­...')
      const [entityResult] = await this.languageClient.analyzeEntities({ document })
      const entities = entityResult.entities || []
      console.log('âœ… ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åˆ†æå®Œäº†:', entities.length, 'ä»¶')

      // ã‚«ãƒ†ã‚´ãƒªåˆ†æ
      console.log('ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ†æå®Ÿè¡Œä¸­...')
      const [categoryResult] = await this.languageClient.classifyText({ document })
      const categories = categoryResult.categories || []
      console.log('âœ… ã‚«ãƒ†ã‚´ãƒªåˆ†æå®Œäº†:', categories.length, 'ä»¶')

      // æ§‹æ–‡åˆ†æ
      console.log('ğŸ”¤ æ§‹æ–‡åˆ†æå®Ÿè¡Œä¸­...')
      const [syntaxResult] = await this.languageClient.analyzeSyntax({ document })
      const tokens = syntaxResult.tokens || []
      const sentences = syntaxResult.sentences || []
      console.log('âœ… æ§‹æ–‡åˆ†æå®Œäº†:', { tokens: tokens.length, sentences: sentences.length })

      const result = {
        sentiment: {
          score: sentiment?.score || 0,
          magnitude: sentiment?.magnitude || 0,
          label: this.getSentimentLabel(sentiment?.score || 0)
        },
        entities: entities.map(entity => ({
          name: entity.name || '',
          type: entity.type || '',
          salience: entity.salience || 0,
          metadata: entity.metadata || {}
        })),
        categories: categories.map(category => ({
          name: category.name || '',
          confidence: category.confidence || 0
        })),
        syntax: {
          sentences: sentences.length,
          tokens: tokens.length,
          avg_sentence_length: sentences.length > 0 ? tokens.length / sentences.length : 0
        }
      }

      console.log('ğŸ‰ NLPåˆ†æå®Œäº†:', result)
      return result

    } catch (error) {
      console.error('âŒ NLPåˆ†æã‚¨ãƒ©ãƒ¼:', error)
      console.error('ã‚¨ãƒ©ãƒ¼è©³ç´°:', {
        message: error instanceof Error ? error.message : 'Unknown error',
        stack: error instanceof Error ? error.stack : undefined,
        textLength: text?.length || 0,
        textPreview: text?.substring(0, 100) + '...' || 'N/A'
      })
      throw error
    }
  }

  // æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«ã®å–å¾—
  private getSentimentLabel(score: number): 'POSITIVE' | 'NEGATIVE' | 'NEUTRAL' {
    if (score > 0.1) return 'POSITIVE'
    if (score < -0.1) return 'NEGATIVE'
    return 'NEUTRAL'
  }

  // BigQueryã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦NLPåˆ†æ
  async analyzeEmailsFromBigQuery(limit: number = 100): Promise<BatchNLPAnalysisResult> {
    const startTime = Date.now()
    
    try {
      // BigQueryã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
      const query = `
        SELECT 
          message_id,
          subject,
          body_preview AS body,
          date
        FROM \`viewpers.salesguard_alerts.email_messages_normalized_v3\`
        WHERE body_preview IS NOT NULL AND LENGTH(TRIM(body_preview)) > 10
        ORDER BY date DESC
        LIMIT ${limit}
      `

      const [rows] = await this.bigquery.query({ query })
      console.log(`ğŸ“§ ${rows.length}ä»¶ã®ãƒ¡ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ`)

      const results: NLPAnalysisResult[] = []
      let successful = 0
      let failed = 0

      // å„ãƒ¡ãƒ¼ãƒ«ã«å¯¾ã—ã¦NLPåˆ†æã‚’å®Ÿè¡Œ
      for (const row of rows) {
        try {
          const nlpResult = await this.analyzeText(row.body)
          
          const result: NLPAnalysisResult = {
            message_id: row.message_id,
            thread_id: row.thread_id, // Assuming thread_id is not directly available in this query, but needed for the type
            subject: row.subject || '',
            body: row.body,
            ...nlpResult,
            analysis_timestamp: new Date().toISOString()
          }

          results.push(result)
          successful++
        } catch (error) {
          console.error(`ãƒ¡ãƒ¼ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼ (${row.message_id}):`, error)
          failed++
        }
      }

      // çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—
      const summary = this.calculateSummary(results)

      return {
        total_processed: rows.length,
        successful_analyses: successful,
        failed_analyses: failed,
        results,
        summary,
        processing_time_ms: Date.now() - startTime
      }

    } catch (error) {
      console.error('BigQueryã‹ã‚‰ã®ãƒ¡ãƒ¼ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼:', error)
      throw error
    }
  }

  // åˆ†æçµæœã®ã‚µãƒãƒªãƒ¼è¨ˆç®—
  private calculateSummary(results: NLPAnalysisResult[]) {
    if (results.length === 0) {
      return {
        sentiment_distribution: {},
        top_entities: [],
        top_categories: [],
        avg_sentiment_score: 0,
        avg_magnitude: 0
      }
    }

    // æ„Ÿæƒ…åˆ†å¸ƒ
    const sentimentCounts: Record<string, number> = {}
    let totalSentimentScore = 0
    let totalMagnitude = 0

    // ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é›†è¨ˆ
    const entityCounts: Record<string, number> = {}
    const categoryCounts: Record<string, number> = {}

    results.forEach(result => {
      // æ„Ÿæƒ…é›†è¨ˆ
      const label = result.sentiment.label
      sentimentCounts[label] = (sentimentCounts[label] || 0) + 1
      totalSentimentScore += result.sentiment.score
      totalMagnitude += result.sentiment.magnitude

      // ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é›†è¨ˆ
      result.entities.forEach(entity => {
        entityCounts[entity.name] = (entityCounts[entity.name] || 0) + 1
      })

      // ã‚«ãƒ†ã‚´ãƒªé›†è¨ˆ
      result.categories.forEach(category => {
        categoryCounts[category.name] = (categoryCounts[category.name] || 0) + 1
      })
    })

    // ãƒˆãƒƒãƒ—ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—
    const topEntities = Object.entries(entityCounts)
      .sort(([, a], [, b]) => b - a)
      .slice(0, 10)
      .map(([name, count]) => ({ name, count }))

    const topCategories = Object.entries(categoryCounts)
      .sort(([, a], [, b]) => b - a)
      .slice(0, 10)
      .map(([name, count]) => ({ name, count }))

    return {
      sentiment_distribution: sentimentCounts,
      top_entities: topEntities,
      top_categories: topCategories,
      avg_sentiment_score: totalSentimentScore / results.length,
      avg_magnitude: totalMagnitude / results.length
    }
  }

  // ç‰¹å®šã®ã‚¹ãƒ¬ãƒƒãƒ‰ã®åˆ†æ
  async analyzeThread(threadId: string): Promise<NLPAnalysisResult[]> {
    try {
      const query = `
        SELECT 
          message_id,
          thread_id,
          subject,
          body,
          date
        FROM \`viewpers.salesguard_alerts.email_messages\`
        WHERE thread_id = @threadId
        ORDER BY date ASC
      `

      const [rows] = await this.bigquery.query({
        query,
        params: { threadId: threadId }
      })

      const results: NLPAnalysisResult[] = []

      for (const row of rows) {
        try {
          const nlpResult = await this.analyzeText(row.body)
          
          const result: NLPAnalysisResult = {
            message_id: row.message_id,
            thread_id: row.thread_id,
            subject: row.subject || '',
            body: row.body,
            ...nlpResult,
            analysis_timestamp: new Date().toISOString()
          }

          results.push(result)
        } catch (error) {
          console.error(`ã‚¹ãƒ¬ãƒƒãƒ‰åˆ†æã‚¨ãƒ©ãƒ¼ (${row.message_id}):`, error)
        }
      }

      return results

    } catch (error) {
      console.error('ã‚¹ãƒ¬ãƒƒãƒ‰åˆ†æã‚¨ãƒ©ãƒ¼:', error)
      throw error
    }
  }

  // åˆ†æçµæœã‚’BigQueryã«ä¿å­˜
  async saveAnalysisResults(results: NLPAnalysisResult[]): Promise<void> {
    try {
      const dataset = this.bigquery.dataset('salesguard_alerts')
      const table = dataset.table('nlp_analysis_results')

      // ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
      const [exists] = await table.exists()
      if (!exists) {
        const schema = [
          { name: 'message_id', type: 'STRING' },
          { name: 'thread_id', type: 'STRING' },
          { name: 'subject', type: 'STRING' },
          { name: 'body', type: 'STRING' },
          { name: 'sentiment_score', type: 'FLOAT64' },
          { name: 'sentiment_magnitude', type: 'FLOAT64' },
          { name: 'sentiment_label', type: 'STRING' },
          { name: 'entities', type: 'STRING' },
          { name: 'categories', type: 'STRING' },
          { name: 'syntax_sentences', type: 'INT64' },
          { name: 'syntax_tokens', type: 'INT64' },
          { name: 'avg_sentence_length', type: 'FLOAT64' },
          { name: 'analysis_timestamp', type: 'TIMESTAMP' }
        ]

        await table.create({ schema })
        console.log('NLPåˆ†æçµæœãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ')
      }

      // ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
      const rows = results.map(result => ({
        message_id: result.message_id,
        thread_id: result.thread_id,
        subject: result.subject,
        body: result.body,
        sentiment_score: result.sentiment.score,
        sentiment_magnitude: result.sentiment.magnitude,
        sentiment_label: result.sentiment.label,
        entities: JSON.stringify(result.entities),
        categories: JSON.stringify(result.categories),
        syntax_sentences: result.syntax.sentences,
        syntax_tokens: result.syntax.tokens,
        avg_sentence_length: result.syntax.avg_sentence_length,
        analysis_timestamp: result.analysis_timestamp
      }))

      await table.insert(rows)
      console.log(`${rows.length}ä»¶ã®NLPåˆ†æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ`)

    } catch (error) {
      console.error('NLPåˆ†æçµæœã®ä¿å­˜ã‚¨ãƒ©ãƒ¼:', error)
      throw error
    }
  }
} 