import fs from 'fs'
import path from 'path'
import { Worker } from 'worker_threads'

interface FastAnalysisOptions {
  chunkSize?: number
  maxSampleRows?: number
  parallel?: boolean
  inferTypes?: boolean
}

interface ColumnStats {
  name: string
  dataType: string
  nullCount: number
  uniqueCount: number
  sampleValues: any[]
  min?: number
  max?: number
  mean?: number
}

interface FastAnalysisResult {
  fileName: string
  totalRows: number
  totalColumns: number
  columns: ColumnStats[]
  processingTime: number
  memoryUsed: number
}

export class FastCSVAnalyzer {
  private options: Required<FastAnalysisOptions>

  constructor(options: FastAnalysisOptions = {}) {
    this.options = {
      chunkSize: options.chunkSize || 10000,
      maxSampleRows: options.maxSampleRows || 1000,
      parallel: options.parallel !== false, // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆtrue
      inferTypes: options.inferTypes !== false
    }
  }

  /**
   * é«˜é€ŸCSVåˆ†æï¼ˆPandasé¢¨ï¼‰
   */
  async analyzeCSV(filePath: string): Promise<FastAnalysisResult> {
    const startTime = Date.now()
    const startMemory = process.memoryUsage().heapUsed
    
    console.log(`ğŸš€ Fast CSV analysis: ${path.basename(filePath)}`)
    console.log(`âš™ï¸  Config: chunk=${this.options.chunkSize}, parallel=${this.options.parallel}`)

    try {
      // ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
      const stats = fs.statSync(filePath)
      const fileSizeMB = (stats.size / (1024 * 1024)).toFixed(2)
      console.log(`ğŸ“Š File size: ${fileSizeMB} MB`)

      let result: FastAnalysisResult

      if (this.options.parallel && stats.size > 50 * 1024 * 1024) { // 50MBä»¥ä¸Šã§ä¸¦åˆ—å‡¦ç†
        console.log(`âš¡ Using parallel processing...`)
        result = await this.analyzeParallel(filePath)
      } else {
        console.log(`ğŸ“‹ Using single-thread processing...`)
        result = await this.analyzeSingleThread(filePath)
      }

      const processingTime = Date.now() - startTime
      const memoryUsed = process.memoryUsage().heapUsed - startMemory

      result.processingTime = processingTime
      result.memoryUsed = memoryUsed

      console.log(`âœ… Analysis completed in ${(processingTime / 1000).toFixed(2)}s`)
      console.log(`ğŸ’¾ Memory used: ${(memoryUsed / (1024 * 1024)).toFixed(2)} MB`)

      return result

    } catch (error) {
      console.error(`âŒ Fast analysis failed:`, error)
      throw error
    }
  }

  /**
   * å˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰é«˜é€Ÿå‡¦ç†
   */
  private async analyzeSingleThread(filePath: string): Promise<FastAnalysisResult> {
    const fileName = path.basename(filePath)
    const chunks: string[][] = []
    let headers: string[] = []
    let totalRows = 0

    // ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿
    console.log(`ğŸ“– Reading file in chunks...`)
    const fileContent = fs.readFileSync(filePath, 'utf8')
    const lines = fileContent.split('\n').filter(line => line.trim())
    
    if (lines.length === 0) {
      throw new Error('Empty CSV file')
    }

    // ãƒ˜ãƒƒãƒ€ãƒ¼å–å¾—
    headers = this.parseCSVLine(lines[0])
    console.log(`ğŸ“‹ Found ${headers.length} columns: ${headers.slice(0, 5).join(', ')}${headers.length > 5 ? '...' : ''}`)

    // ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
    for (let i = 1; i < lines.length; i += this.options.chunkSize) {
      const chunkLines = lines.slice(i, i + this.options.chunkSize)
      const chunkData = chunkLines.map(line => this.parseCSVLine(line))
      chunks.push(...chunkData)
      totalRows += chunkData.length

      if (i % (this.options.chunkSize * 10) === 1) {
        console.log(`ğŸ“Š Processed ${Math.min(i + this.options.chunkSize, lines.length - 1)}/${lines.length - 1} rows`)
      }
    }

    console.log(`ğŸ” Analyzing column statistics...`)
    const columns = await this.analyzeColumns(headers, chunks)

    return {
      fileName,
      totalRows,
      totalColumns: headers.length,
      columns,
      processingTime: 0, // å¾Œã§è¨­å®š
      memoryUsed: 0      // å¾Œã§è¨­å®š
    }
  }

  /**
   * ä¸¦åˆ—å‡¦ç†ç‰ˆï¼ˆWorkerä½¿ç”¨ï¼‰
   */
  private async analyzeParallel(filePath: string): Promise<FastAnalysisResult> {
    const fileName = path.basename(filePath)
    
    console.log(`ğŸ”§ Setting up parallel workers...`)
    
    // ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¤‡æ•°ã®éƒ¨åˆ†ã«åˆ†å‰²ã—ã¦ä¸¦åˆ—å‡¦ç†
    const fileContent = fs.readFileSync(filePath, 'utf8')
    const lines = fileContent.split('\n').filter(line => line.trim())
    
    if (lines.length === 0) {
      throw new Error('Empty CSV file')
    }

    const headers = this.parseCSVLine(lines[0])
    const dataLines = lines.slice(1)
    
    // ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ±ºå®šï¼ˆCPUã‚³ã‚¢æ•°ã«åŸºã¥ãï¼‰
    const numWorkers = Math.min(4, Math.ceil(dataLines.length / this.options.chunkSize))
    const chunkSize = Math.ceil(dataLines.length / numWorkers)
    
    console.log(`ğŸ‘¥ Using ${numWorkers} workers, ${chunkSize} rows per worker`)

    const workerPromises: Promise<ColumnStats[]>[] = []

    for (let i = 0; i < numWorkers; i++) {
      const startIdx = i * chunkSize
      const endIdx = Math.min(startIdx + chunkSize, dataLines.length)
      const workerChunk = dataLines.slice(startIdx, endIdx)
      
      workerPromises.push(this.processChunkInWorker(headers, workerChunk))
    }

    console.log(`â³ Waiting for workers to complete...`)
    const workerResults = await Promise.all(workerPromises)

    // çµæœã‚’ãƒãƒ¼ã‚¸
    console.log(`ğŸ”„ Merging results from ${numWorkers} workers...`)
    const columns = this.mergeColumnStats(headers, workerResults)

    return {
      fileName,
      totalRows: dataLines.length,
      totalColumns: headers.length,
      columns,
      processingTime: 0, // å¾Œã§è¨­å®š
      memoryUsed: 0      // å¾Œã§è¨­å®š
    }
  }

  /**
   * ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
   */
  private async processChunkInWorker(headers: string[], chunk: string[]): Promise<ColumnStats[]> {
    return new Promise((resolve, reject) => {
      // ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ãƒ»ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚³ãƒ¼ãƒ‰
      const workerCode = `
        const { parentPort } = require('worker_threads');
        
        parentPort.on('message', ({ headers, chunk }) => {
          try {
            const stats = headers.map(header => ({
              name: header,
              dataType: 'string',
              nullCount: 0,
              uniqueCount: 0,
              sampleValues: [],
              min: undefined,
              max: undefined,
              mean: undefined
            }));

            const uniqueSets = headers.map(() => new Set());
            const numericValues = headers.map(() => []);

            chunk.forEach(row => {
              row.forEach((value, colIdx) => {
                if (!value || value.trim() === '') {
                  stats[colIdx].nullCount++;
                } else {
                  uniqueSets[colIdx].add(value);
                  
                  // æ•°å€¤åˆ¤å®š
                  const numValue = parseFloat(value);
                  if (!isNaN(numValue)) {
                    numericValues[colIdx].push(numValue);
                  }
                  
                  // ã‚µãƒ³ãƒ—ãƒ«å€¤åé›†
                  if (stats[colIdx].sampleValues.length < 10) {
                    stats[colIdx].sampleValues.push(value);
                  }
                }
              });
            });

            // çµ±è¨ˆè¨ˆç®—
            stats.forEach((stat, idx) => {
              stat.uniqueCount = uniqueSets[idx].size;
              
              const numVals = numericValues[idx];
              if (numVals.length > 0) {
                stat.dataType = 'number';
                stat.min = Math.min(...numVals);
                stat.max = Math.max(...numVals);
                stat.mean = numVals.reduce((a, b) => a + b, 0) / numVals.length;
              }
            });

            parentPort.postMessage({ success: true, stats });
          } catch (error) {
            parentPort.postMessage({ success: false, error: error.message });
          }
        });
      `;

      const worker = new Worker(workerCode, { eval: true })
      
      worker.postMessage({ headers, chunk: chunk.map(line => this.parseCSVLine(line)) })
      
      worker.on('message', (result) => {
        worker.terminate()
        if (result.success) {
          resolve(result.stats)
        } else {
          reject(new Error(result.error))
        }
      })

      worker.on('error', (error) => {
        worker.terminate()
        reject(error)
      })
    })
  }

  /**
   * è¤‡æ•°ãƒ¯ãƒ¼ã‚«ãƒ¼ã®çµæœã‚’ãƒãƒ¼ã‚¸
   */
  private mergeColumnStats(headers: string[], workerResults: ColumnStats[][]): ColumnStats[] {
    const merged: ColumnStats[] = headers.map(header => ({
      name: header,
      dataType: 'string',
      nullCount: 0,
      uniqueCount: 0,
      sampleValues: [],
      min: undefined,
      max: undefined,
      mean: undefined
    }))

    workerResults.forEach(workerStats => {
      workerStats.forEach((stat, idx) => {
        merged[idx].nullCount += stat.nullCount
        merged[idx].sampleValues.push(...stat.sampleValues.slice(0, 3)) // å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‹ã‚‰3å€‹ãšã¤
        
        if (stat.dataType === 'number') {
          merged[idx].dataType = 'number'
          merged[idx].min = merged[idx].min ? Math.min(merged[idx].min!, stat.min!) : stat.min
          merged[idx].max = merged[idx].max ? Math.max(merged[idx].max!, stat.max!) : stat.max
        }
      })
    })

    // ã‚µãƒ³ãƒ—ãƒ«å€¤ã‚’åˆ¶é™
    merged.forEach(stat => {
      stat.sampleValues = stat.sampleValues.slice(0, 10)
      stat.uniqueCount = new Set(stat.sampleValues).size // è¿‘ä¼¼å€¤
    })

    return merged
  }

  /**
   * åˆ—çµ±è¨ˆåˆ†æ
   */
  private async analyzeColumns(headers: string[], data: string[][]): Promise<ColumnStats[]> {
    const columns: ColumnStats[] = headers.map(header => ({
      name: header,
      dataType: 'string',
      nullCount: 0,
      uniqueCount: 0,
      sampleValues: []
    }))

    const uniqueSets = headers.map(() => new Set<string>())
    const numericValues = headers.map(() => [] as number[])

    // ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‡¦ç†
    const sampleSize = Math.min(this.options.maxSampleRows, data.length)
    const sampleData = data.slice(0, sampleSize)

    console.log(`ğŸ“Š Analyzing ${sampleSize} sample rows...`)

    sampleData.forEach((row, rowIdx) => {
      if (rowIdx % 1000 === 0) {
        console.log(`ğŸ” Processing row ${rowIdx + 1}/${sampleSize}`)
      }

      row.forEach((value, colIdx) => {
        if (colIdx >= columns.length) return

        if (!value || value.trim() === '') {
          columns[colIdx].nullCount++
        } else {
          uniqueSets[colIdx].add(value)
          
          // æ•°å€¤åˆ¤å®š
          const numValue = parseFloat(value)
          if (!isNaN(numValue)) {
            numericValues[colIdx].push(numValue)
          }
          
          // ã‚µãƒ³ãƒ—ãƒ«å€¤åé›†
          if (columns[colIdx].sampleValues.length < 10) {
            columns[colIdx].sampleValues.push(value)
          }
        }
      })
    })

    // çµ±è¨ˆè¨ˆç®—
    columns.forEach((column, idx) => {
      column.uniqueCount = uniqueSets[idx].size
      
      const numVals = numericValues[idx]
      if (numVals.length > 0 && numVals.length / sampleSize > 0.8) { // 80%ä»¥ä¸ŠãŒæ•°å€¤
        column.dataType = 'number'
        column.min = Math.min(...numVals)
        column.max = Math.max(...numVals)
        column.mean = numVals.reduce((a, b) => a + b, 0) / numVals.length
      }
    })

    return columns
  }

  /**
   * CSVè¡Œè§£æï¼ˆç°¡æ˜“ç‰ˆï¼‰
   */
  private parseCSVLine(line: string): string[] {
    // ç°¡æ˜“CSVè§£æï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€ã‚¯ã‚©ãƒ¼ãƒˆæœªå¯¾å¿œï¼‰
    return line.split(',').map(cell => cell.trim())
  }
}

/**
 * ä½¿ç”¨ä¾‹ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
 */
export async function benchmarkCSVAnalysis(filePath: string) {
  console.log(`ğŸ Starting CSV analysis benchmark for ${path.basename(filePath)}`)
  
  // å¾“æ¥ã®æ–¹æ³•
  console.log(`\nğŸ“Š Method 1: Traditional row-by-row`)
  const start1 = Date.now()
  // const result1 = await traditionalAnalysis(filePath)
  const time1 = Date.now() - start1
  
  // é«˜é€Ÿæ–¹æ³•
  console.log(`\nğŸš€ Method 2: Fast chunked analysis`)
  const start2 = Date.now()
  const analyzer = new FastCSVAnalyzer({ parallel: false })
  const result2 = await analyzer.analyzeCSV(filePath)
  const time2 = Date.now() - start2
  
  // ä¸¦åˆ—æ–¹æ³•
  console.log(`\nâš¡ Method 3: Parallel analysis`)
  const start3 = Date.now()
  const parallelAnalyzer = new FastCSVAnalyzer({ parallel: true })
  const result3 = await parallelAnalyzer.analyzeCSV(filePath)
  const time3 = Date.now() - start3
  
  console.log(`\nğŸ“ˆ Benchmark Results:`)
  console.log(`Traditional: ${(time1 / 1000).toFixed(2)}s`)
  console.log(`Fast Single: ${(time2 / 1000).toFixed(2)}s (${(time1 / time2).toFixed(2)}x faster)`)
  console.log(`Fast Parallel: ${(time3 / 1000).toFixed(2)}s (${(time1 / time3).toFixed(2)}x faster)`)
  
  return { result2, result3 }
} 