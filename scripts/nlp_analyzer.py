#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import json
import time
from transformers import pipeline
import torch

class JapaneseSentimentAnalyzer:
    def __init__(self):
        """è»½é‡æ—¥æœ¬èªæ„Ÿæƒ…åˆ†æå™¨ã®åˆæœŸåŒ–"""
        print("ğŸ¤– æ—¥æœ¬èªæ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...", file=sys.stderr)
        
        try:
            # è»½é‡æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            self.sentiment_analyzer = pipeline(
                "sentiment-analysis",
                model="cl-tohoku/bert-base-japanese-v3",
                device=0 if torch.cuda.is_available() else -1
            )
            
            # æ„Ÿæƒ…ã‚«ãƒ†ã‚´ãƒªåˆ†é¡å™¨ï¼ˆã‚«ã‚¹ã‚¿ãƒ ï¼‰
            self.emotion_classifier = pipeline(
                "text-classification",
                model="cl-tohoku/bert-base-japanese-v3"
            )
            
            print("âœ… ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†ï¼", file=sys.stderr)
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}", file=sys.stderr)
            raise
    
    def analyze(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…åˆ†æã‚’å®Ÿè¡Œ"""
        start_time = time.time()
        
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ï¼ˆé•·ã•åˆ¶é™ï¼‰
            processed_text = text[:512] if len(text) > 512 else text
            
            # åŸºæœ¬æ„Ÿæƒ…åˆ†æ
            sentiment_result = self.sentiment_analyzer(processed_text)
            
            # æ„Ÿæƒ…ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
            emotion_result = self.emotion_classifier(processed_text)
            
            # å‡¦ç†æ™‚é–“è¨ˆç®—
            processing_time = (time.time() - start_time) * 1000
            
            # çµæœçµ±åˆ
            result = {
                'sentiment': sentiment_result[0]['label'],
                'sentiment_confidence': float(sentiment_result[0]['score']),
                'emotion': emotion_result[0]['label'],
                'emotion_confidence': float(emotion_result[0]['score']),
                'text_length': len(text),
                'processed_text_length': len(processed_text),
                'processing_time_ms': int(processing_time),
                'processed': True,
                'model_version': 'cl-tohoku/bert-base-japanese-v3',
                'timestamp': time.time()
            }
            
            return result
            
        except Exception as e:
            return {
                'error': str(e),
                'processed': False,
                'processing_time_ms': int((time.time() - start_time) * 1000)
            }
    
    def analyze_batch(self, texts):
        """è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã®ä¸€æ‹¬æ„Ÿæƒ…åˆ†æ"""
        results = []
        
        for i, text in enumerate(texts):
            print(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆ {i+1}/{len(texts)} ã‚’å‡¦ç†ä¸­...", file=sys.stderr)
            result = self.analyze(text)
            results.append(result)
        
        return results

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    analyzer = JapaneseSentimentAnalyzer()
    
    print("ğŸš€ æ—¥æœ¬èªæ„Ÿæƒ…åˆ†æå™¨ãŒèµ·å‹•ã—ã¾ã—ãŸ", file=sys.stderr)
    print("ğŸ“ æ¨™æº–å…¥åŠ›ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã¾ã™", file=sys.stderr)
    
    # æ¨™æº–å…¥åŠ›ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿è¾¼ã¿
    for line in sys.stdin:
        try:
            data = json.loads(line.strip())
            text = data.get('text', '')
            
            if not text:
                continue
            
            # æ„Ÿæƒ…åˆ†æå®Ÿè¡Œ
            result = analyzer.analyze(text)
            
            # çµæœã‚’æ¨™æº–å‡ºåŠ›ã«å‡ºåŠ›
            print(json.dumps(result, ensure_ascii=False))
            sys.stdout.flush()
            
        except json.JSONDecodeError:
            print("âŒ JSONå½¢å¼ãŒç„¡åŠ¹ã§ã™", file=sys.stderr)
            continue
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™", file=sys.stderr)
            break
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
            continue

if __name__ == "__main__":
    main() 