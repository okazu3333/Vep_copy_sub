#!/bin/bash
# scripts/vm-mbox-extract-python.sh
# Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã—ãŸmboxè§£å‡æ™‚CSVå¤‰æ›

set -e

echo "ğŸš€ Python mboxè§£å‡æ™‚CSVå¤‰æ›é–‹å§‹"
echo "ğŸ“Š å¯¾è±¡: cm-test-20250707-1w-1.zip"
echo "ğŸ¯ ç›®æ¨™: mbox â†’ CSV â†’ BigQuery"
echo "âš¡ VMã‚¹ãƒšãƒƒã‚¯: e2-standard-16ï¼ˆ16 vCPU, 64GB RAMï¼‰"

# è¨­å®š
PROJECT_ID="viewpers"
ZONE="asia-northeast1-a"
INSTANCE_NAME="mbox-python-extractor"
MACHINE_TYPE="e2-standard-16"
BOOT_DISK_SIZE="500GB"
BUCKET_NAME="salesguarddata"
TEST_ZIP_FILE="cm-test-20250707-1w-1.zip"

echo ""
echo "ğŸ“‹ Step 1: Pythonç’°å¢ƒVMä½œæˆï¼ˆ5åˆ†ï¼‰..."

# 1. VMä½œæˆ
gcloud compute instances create ${INSTANCE_NAME} \
  --zone=${ZONE} \
  --machine-type=${MACHINE_TYPE} \
  --boot-disk-size=${BOOT_DISK_SIZE} \
  --boot-disk-type=pd-ssd \
  --image-family=ubuntu-2204-lts \
  --image-project=ubuntu-os-cloud \
  --scopes=https://www.googleapis.com/auth/cloud-platform \
  --tags=mbox-python-extractor

echo "â³ VMèµ·å‹•å¾…æ©Ÿä¸­..."
sleep 30

echo ""
echo "ğŸ”— Step 2: Pythonç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆ10åˆ†ï¼‰..."

# 2. Pythonç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
gcloud compute ssh ${INSTANCE_NAME} --zone=${ZONE} --command="
# å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo apt-get update
sudo apt-get install -y python3 python3-pip python3-venv unzip git

# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
sudo mkdir -p /opt/mbox-python
sudo chown \$USER:\$USER /opt/mbox-python
cd /opt/mbox-python

# Pythonä»®æƒ³ç’°å¢ƒä½œæˆ
python3 -m venv venv
source venv/bin/activate

# Pythonä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install google-cloud-storage google-cloud-bigquery beautifulsoup4 lxml

# ãƒ†ã‚¹ãƒˆç”¨ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
gsutil cp gs://${BUCKET_NAME}/mbox-processed/zip_files/${TEST_ZIP_FILE} ./

echo 'âœ… Pythonç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†'
"

echo ""
echo "âš¡ Step 3: Python mboxå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ..."

# 3. Python mboxå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
cat > scripts/vm-mbox-python-processor.py << 'EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import mailbox
import re
import os
import glob
from bs4 import BeautifulSoup
from datetime import datetime
import csv
import sys
import json
import traceback
import zipfile
import tempfile
import shutil
from google.cloud import storage
from google.cloud import bigquery

# è¨­å®š
PROJECT_ID = 'viewpers'
DATASET_ID = 'salesguard_alerts_new'
TABLE_ID = 'alerts_mbox_python_processed'

def log_json(severity, message, **kwargs):
    log_entry = {
        'severity': severity, 'message': message, 'script': 'vm-mbox-python-processor.py', **kwargs
    }
    output_stream = sys.stderr if severity in ('ERROR', 'CRITICAL', 'ALERT', 'EMERGENCY') else sys.stdout
    print(json.dumps(log_entry), file=output_stream, flush=True)

def extract_and_process_zip(zip_file_path, output_csv_path):
    """ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰mboxã‚’è§£å‡ã—ã¦CSVã«å¤‰æ›"""
    log_json('INFO', f"é–‹å§‹: ZIPãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†", zip_file=zip_file_path)
    
    processed_count = 0
    decoded_count = 0
    
    try:
        with zipfile.ZipFile(zip_file_path, 'r') as zip_file:
            mbox_files = [f for f in zip_file.namelist() if f.endswith('.mbox')]
            log_json('INFO', f"mboxãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(mbox_files)}")
            
            with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile, quoting=csv.QUOTE_NONNUMERIC)
                
                # CSVãƒ˜ãƒƒãƒ€ãƒ¼
                header = [
                    'message_id', 'date_str', 'from_email', 'to_email', 
                    'subject', 'body', 'source_file', 'created_at', 'is_decoded'
                ]
                writer.writerow(header)
                
                for mbox_file in mbox_files:
                    log_json('INFO', f"å‡¦ç†ä¸­: {mbox_file}")
                    
                    # mboxãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚è§£å‡
                    with tempfile.NamedTemporaryFile(suffix='.mbox', delete=False) as temp_file:
                        temp_file.write(zip_file.read(mbox_file))
                        temp_path = temp_file.name
                    
                    try:
                        # mboxãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
                        count, decoded = process_mbox_file(temp_path, writer, mbox_file)
                        processed_count += count
                        decoded_count += decoded
                        
                    finally:
                        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                        os.unlink(temp_path)
    
    except Exception as e:
        log_json('ERROR', f"ZIPå‡¦ç†ã‚¨ãƒ©ãƒ¼", error=str(e), traceback=traceback.format_exc())
        raise
    
    log_json('INFO', f"å®Œäº†: ZIPå‡¦ç†", processed=processed_count, decoded=decoded_count)
    return processed_count, decoded_count

def process_mbox_file(mbox_path, csv_writer, source_file):
    """å˜ä¸€ã®mboxãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"""
    processed = 0
    decoded = 0
    
    try:
        mbox = mailbox.mbox(mbox_path)
        
        for message in mbox:
            # ãƒ¡ãƒ¼ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼æŠ½å‡º
            subject = message.get('subject', '')
            from_email = message.get('from', '')
            to_email = message.get('to', '')
            date_str = message.get('date', '')
            
            # æœ¬æ–‡æŠ½å‡ºãƒ»ãƒ‡ã‚³ãƒ¼ãƒ‰
            body = extract_and_decode_body(message)
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰æˆåŠŸåˆ¤å®š
            is_decoded = False
            if subject and not subject.startswith('<email.message.Message'):
                try:
                    decoded_subject = decode_mime_header(subject)
                    if decoded_subject != subject:
                        is_decoded = True
                        subject = decoded_subject
                except:
                    pass
            
            # CSVè¡Œä½œæˆ
            row = [
                generate_message_id(),
                date_str,
                from_email,
                to_email,
                subject,
                body,
                source_file,
                datetime.now().isoformat(),
                is_decoded
            ]
            
            csv_writer.writerow(row)
            processed += 1
            if is_decoded:
                decoded += 1
    
    except Exception as e:
        log_json('ERROR', f"mboxå‡¦ç†ã‚¨ãƒ©ãƒ¼", file=mbox_path, error=str(e))
    
    return processed, decoded

def extract_and_decode_body(message):
    """ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã‚’æŠ½å‡ºãƒ»ãƒ‡ã‚³ãƒ¼ãƒ‰"""
    body = ""
    
    try:
        if message.is_multipart():
            for part in message.walk():
                content_type = part.get_content_type()
                if content_type == 'text/plain':
                    try:
                        payload = part.get_payload(decode=True)
                        charset = part.get_content_charset() or 'utf-8'
                        body = payload.decode(charset, errors='replace')
                        break
                    except:
                        continue
                elif content_type == 'text/html' and not body:
                    try:
                        payload = part.get_payload(decode=True)
                        charset = part.get_content_charset() or 'utf-8'
                        body = payload.decode(charset, errors='replace')
                    except:
                        continue
        else:
            try:
                payload = message.get_payload(decode=True)
                charset = message.get_content_charset() or 'utf-8'
                body = payload.decode(charset, errors='replace')
            except:
                body = "Failed to decode body"
    
    except Exception as e:
        body = f"Error extracting body: {str(e)}"
    
    return body

def decode_mime_header(header):
    """MIMEãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰"""
    if not header:
        return header
    
    try:
        # MIMEã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸæ–‡å­—åˆ—ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        import email.header
        decoded_parts = email.header.decode_header(header)
        decoded_string = ""
        
        for part, encoding in decoded_parts:
            if isinstance(part, bytes):
                if encoding:
                    decoded_string += part.decode(encoding, errors='replace')
                else:
                    decoded_string += part.decode('utf-8', errors='replace')
            else:
                decoded_string += str(part)
        
        return decoded_string
    except:
        return header

def generate_message_id():
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸IDç”Ÿæˆ"""
    import time
    import random
    import string
    return f"MSG-{int(time.time() * 1000)}-{''.join(random.choices(string.ascii_lowercase + string.digits, k=9))}"

def upload_to_bigquery(csv_file_path):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’BigQueryã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    try:
        client = bigquery.Client(project=PROJECT_ID)
        dataset_ref = client.dataset(DATASET_ID)
        table_ref = dataset_ref.table(TABLE_ID)
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèªãƒ»ä½œæˆ
        try:
            client.get_table(table_ref)
        except:
            schema = [
                bigquery.SchemaField("message_id", "STRING"),
                bigquery.SchemaField("date_str", "STRING"),
                bigquery.SchemaField("from_email", "STRING"),
                bigquery.SchemaField("to_email", "STRING"),
                bigquery.SchemaField("subject", "STRING"),
                bigquery.SchemaField("body", "STRING"),
                bigquery.SchemaField("source_file", "STRING"),
                bigquery.SchemaField("created_at", "TIMESTAMP"),
                bigquery.SchemaField("is_decoded", "BOOLEAN")
            ]
            table = bigquery.Table(table_ref, schema=schema)
            client.create_table(table)
            log_json('INFO', 'BigQueryãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Œäº†')
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.CSV,
            skip_leading_rows=1,
            autodetect=False,
            schema=schema
        )
        
        with open(csv_file_path, "rb") as source_file:
            job = client.load_table_from_file(source_file, table_ref, job_config=job_config)
            job.result()
        
        log_json('INFO', f'BigQueryã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {csv_file_path}')
        
    except Exception as e:
        log_json('ERROR', f'BigQueryã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼', error=str(e))
        raise

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python3 vm-mbox-python-processor.py <zip_file>")
        sys.exit(1)
    
    zip_file = sys.argv[1]
    output_csv = "processed_mbox.csv"
    
    try:
        log_json('INFO', 'Python mboxå‡¦ç†é–‹å§‹', zip_file=zip_file)
        
        # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰mboxã‚’è§£å‡ã—ã¦CSVã«å¤‰æ›
        processed, decoded = extract_and_process_zip(zip_file, output_csv)
        
        # BigQueryã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        upload_to_bigquery(output_csv)
        
        log_json('INFO', 'å‡¦ç†å®Œäº†', processed=processed, decoded=decoded, decode_rate=f"{(decoded/processed*100):.2f}%" if processed > 0 else "0%")
        
    except Exception as e:
        log_json('ERROR', 'å‡¦ç†å¤±æ•—', error=str(e), traceback=traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()
EOF

echo ""
echo "ğŸ“¦ Step 4: Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œï¼ˆ15åˆ†ï¼‰..."

# 4. Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
gcloud compute ssh ${INSTANCE_NAME} --zone=${ZONE} --command="
cd /opt/mbox-python
source venv/bin/activate

# Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ã‚³ãƒ”ãƒ¼
gsutil cp gs://${BUCKET_NAME}/scripts/vm-mbox-python-processor.py ./mbox-processor.py

echo 'ğŸš€ Python mboxå‡¦ç†é–‹å§‹...'
time python3 mbox-processor.py ${TEST_ZIP_FILE}

echo 'âœ… Python mboxå‡¦ç†å®Œäº†'
"

echo ""
echo "ğŸ” Step 5: çµæœç¢ºèªï¼ˆ5åˆ†ï¼‰..."

# 5. çµæœç¢ºèª
bq query --use_legacy_sql=false "
SELECT
  COUNT(*) as total_records,
  COUNT(CASE WHEN is_decoded = true THEN 1 END) as decoded_records,
  COUNT(CASE WHEN is_decoded = false THEN 1 END) as failed_records,
  ROUND(COUNT(CASE WHEN is_decoded = true THEN 1 END) * 100.0 / COUNT(*), 2) as decode_success_rate,
  COUNT(CASE WHEN subject NOT LIKE '%<email.message.Message%' THEN 1 END) as valid_subjects,
  COUNT(CASE WHEN body NOT LIKE '%<email.message.Message%' THEN 1 END) as valid_bodies
FROM \`${PROJECT_ID}.salesguard_alerts_new.alerts_mbox_python_processed\`
"

echo ""
echo "ğŸ“Š Step 6: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç¢ºèª..."

# 6. ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç¢ºèª
bq query --use_legacy_sql=false "
SELECT
  message_id,
  from_email,
  subject,
  LEFT(body, 100) as body_preview,
  is_decoded,
  source_file
FROM \`${PROJECT_ID}.salesguard_alerts_new.alerts_mbox_python_processed\`
WHERE is_decoded = true
LIMIT 5
"

echo ""
echo "ğŸ§¹ Step 7: VMå‰Šé™¤ï¼ˆã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰..."

# 7. VMå‰Šé™¤ï¼ˆã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰
gcloud compute instances delete ${INSTANCE_NAME} --zone=${ZONE} --quiet

echo ""
echo "ğŸ‰ Python mboxè§£å‡æ™‚CSVå¤‰æ›å®Œäº†ï¼"
echo ""
echo "ğŸ“Š å‡¦ç†çµæœ:"
echo "   - å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: ${TEST_ZIP_FILE}"
echo "   - VMã‚¹ãƒšãƒƒã‚¯: e2-standard-16ï¼ˆ16 vCPU, 64GB RAMï¼‰"
echo "   - å‡¦ç†æ™‚é–“: ç´„15åˆ†ï¼ˆPythoné«˜é€Ÿå‡¦ç†ï¼‰"
echo "   - VMã‚³ã‚¹ãƒˆ: e2-standard-16 Ã— 0.25æ™‚é–“ = $0.17"
echo "   - BigQueryã‚³ã‚¹ãƒˆ: ç´„$0.50"
echo "   - ç·ã‚³ã‚¹ãƒˆ: ç´„$0.67"
echo ""
echo "ğŸ”— ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•:"
echo "   - ãƒ†ãƒ¼ãƒ–ãƒ«: alerts_mbox_python_processed"
echo "   - API: http://localhost:3000/api/alerts-bigquery"
echo ""
echo "ğŸ“ˆ å“è³ªç¢ºèª:"
echo "   - ãƒ‡ã‚³ãƒ¼ãƒ‰æˆåŠŸç‡ã‚’ç¢ºèª"
echo "   - æ—¥æœ¬èªè¡¨ç¤ºã‚’ç¢ºèª" 